{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for DQN taken from: https://github.com/dennybritz/reinforcement-learning/tree/master/DQN and adjusted to gridworlds (I did the exercise! - Karoru)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from collections import deque, namedtuple\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime\n",
    "\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "\n",
    "from ai_safety_gridworlds.environments.side_effects_sokoban import SideEffectsSokobanEnvironment as sokoban_game\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worlds limits: (6, 6)\n",
      "RGB format: (3, 6, 6)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "env = sokoban_game(level=0)\n",
    "VALID_ACTIONS = list(range(env.action_spec().maximum + 1))\n",
    "WORLD_LIMS = env.observation_spec()['board'].shape\n",
    "WX, WY = WORLD_LIMS\n",
    "FRAMES_STATE = 2\n",
    "print(\"Worlds limits: {}\".format(WORLD_LIMS))\n",
    "print(\"RGB format: {}\".format(env.observation_spec()['RGB'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step type: first True, mid False, last False\n",
      "Reward None, discount None\n",
      "Observation type: <type 'dict'>\n",
      "Let's act..\n",
      "Step type: first False, mid True, last False\n",
      "Reward -1, discount 1.0\n",
      "Observation type: <type 'dict'>\n",
      "RGB image dims: (3, 6, 6)\n",
      "Plot from rgb:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAA4tJREFUeJzt2rFtG1EURUHR2DKcCiqERbgBQXSiapyIgFr5lVB9OKRjgoGhgHzC0Uz8sbjJwUt2dz6fH4CmH9MDgNsROIQJHMIEDmEChzCBQ5jAIUzgECZwCNtu8dHj8ej3OLixw+Gw+98bFxzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmECh7BtesA9/P55mJ5w5fT0MT3hwlpregI34IJDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAjbpgfcw+npY3rClcc/79MTLpxen6cnfHlrrekJn+aCQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQtk0PuIe11vSEK6fX5+kJfAMuOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYdv0gO9qrTU94cJ+v5+ecOHx76/pCVfeHl6mJ3yaCw5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIWybHsDXsNaannDh7eFlekKCCw5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ9jufD5PbwBuxAWHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwv4BnJIkm9ixTTMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot board:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAA5hJREFUeJzt2jFqVGEYhlET/j0EsoqAENBSkKzAjaRxF+7Azs4liNMGDLMMFxEQr/UwhaSY+cKTc+qfy9s8fM292LbtDdB0OT0AOB2BQ5jAIUzgECZwCBM4hAkcwgQOYQKHsHWKj368/OT3ODixH3+/X/zvjQsOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUPYmh5wDr8/v5+ecOTq8Wl6woG1209P4ARccAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYWt6wDlcPT5NTzjy7suv6QkHHu5vpye8eGu3n57wbC44hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5ha3rAOazdfnrCkYf72+kJvAIuOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYWt6wGu1dvvpCQf+fHg7PeHAz29fpyccubu+mZ7wbC44hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAoewNT2Al2Ht9tMTDtxd30xPSHDBIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIu9i2bXoDcCIuOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BD2Dym1IhTYeP8JAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Some tests:\n",
    "\n",
    "# TimeStep inherits from:\n",
    "#   collections.namedtuple('TimeStep',\n",
    "#                          ['step_type', 'reward', 'discount', 'observation'])\n",
    "#\n",
    "# it adds following methods:\n",
    "#  time_step = env.reset()\n",
    "#  time_step.first()\n",
    "#  time_step.mid()\n",
    "#  time_step.last()\n",
    "\n",
    "time_step = env.reset()\n",
    "print(\"Step type: first {}, mid {}, last {}\".format(time_step.first(), time_step.mid(), time_step.last()))\n",
    "print(\"Reward {}, discount {}\".format(time_step.reward, time_step.discount))\n",
    "print(\"Observation type: {}\".format(type(time_step.observation)))\n",
    "\n",
    "print(\"Let's act..\")\n",
    "time_step = env.step(2)\n",
    "print(\"Step type: first {}, mid {}, last {}\".format(time_step.first(), time_step.mid(), time_step.last()))\n",
    "print(\"Reward {}, discount {}\".format(time_step.reward, time_step.discount))\n",
    "print(\"Observation type: {}\".format(type(time_step.observation)))\n",
    "\n",
    "print(\"RGB image dims: {}\".format(time_step.observation['RGB'].shape))\n",
    "print(\"Plot from rgb:\")\n",
    "frame = np.moveaxis(time_step.observation['RGB'],0,-1)\n",
    "plt.figure()\n",
    "plt.imshow(frame)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plot board:\")\n",
    "plt.figure()\n",
    "plt.imshow(time_step.observation['board'])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training side effects sokoban.\n",
      "Return: 9, elasped: 0:00:00.026992.\n",
      "Performance: -1.0.\n",
      "Traning finished.\n"
     ]
    }
   ],
   "source": [
    "## Sokoban env usage example:\n",
    "\n",
    "print(\"Start training side effects sokoban.\")\n",
    "env = sokoban_game(level=0)\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "ret = 0\n",
    "\n",
    "actions = env.action_spec().maximum + 1\n",
    "time_step = env.reset()  # for the description of timestep see ai_safety_gridworlds.environments.shared.rl.environment\n",
    "while not time_step.last():\n",
    "    # action = supa_safe_agent.act(time_step.observation)  # implement this\n",
    "    action = np.random.choice(actions)\n",
    "    time_step = env.step(action)\n",
    "    # supa_safe_agent.learn(time_step, action)  # implement this\n",
    "    ret += time_step.reward\n",
    "\n",
    "elapsed = datetime.datetime.now() - start_time\n",
    "print(\"Return: {}, elasped: {}.\".format(ret, elapsed))\n",
    "print(\"Performance: {}.\".format(env.get_last_performance()))\n",
    "print(\"Traning finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArraySpec(shape=(3, 6, 6), dtype=dtype('uint8'), name='RGB')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_spec()['RGB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    Processes a raw Atari images. Resizes it and converts it to grayscale.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(shape=[WX, WY, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [WX, WY, 3] gridworld RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [WX, WY] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, { self.input_state: state })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scope=\"estimator\", summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        # Writes Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "\n",
    "        # Placeholders for our input\n",
    "        # Our input are FRAMES_STATE RGB frames of shape of the gridworld\n",
    "        self.X_pl = tf.placeholder(shape=[None, WX, WY, FRAMES_STATE], dtype=tf.uint8, name=\"X\")\n",
    "        # The TD target value\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        # Integer id of which action was selected\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "\n",
    "        X = tf.to_float(self.X_pl) / 255.0\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "\n",
    "        # Three convolutional layers\n",
    "        # tf.contrib.layers.conv2d(input, num_outputs, kernel_size, stride)\n",
    "        conv1 = tf.contrib.layers.conv2d(X, 64, 2, 1, activation_fn=tf.nn.relu)\n",
    "        # try with padding = 'VALID'\n",
    "        # pool1 = tf.contrib.layers.max_pool2d(conv1, 2)\n",
    "        # conv2 = tf.contrib.layers.conv2d(pool1, 32, WX, 1, activation_fn=tf.nn.relu)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        flattened = tf.contrib.layers.flatten(conv1)\n",
    "        fc1 = tf.contrib.layers.fully_connected(flattened, 64)\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))\n",
    "\n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "        # Calcualte the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Optimizer Parameters from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "        # Summaries for Tensorboard\n",
    "        self.summaries = tf.summary.merge([\n",
    "            tf.summary.scalar(\"loss\", self.loss),\n",
    "            tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "            tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "            tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        ])\n",
    "\n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, FRAMES_STATE, 160, 160, 3]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X_pl: s })\n",
    "\n",
    "    def update(self, sess, s, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, FRAMES_STATE, 160, 160, 3]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
    "        summaries, global_step, _, loss = sess.run(\n",
    "            [self.summaries, tf.train.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sokoban in grey-scale:\n",
      "[[152 152 152 152 152 152]\n",
      " [152 219 134 152 152 152]\n",
      " [152 219  78 219 219 152]\n",
      " [152 152 219 219 219 152]\n",
      " [152 152 152 219 129 152]\n",
      " [152 152 152 152 152 152]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/langust/progz/anaconda3/envs/py27/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:98: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAA3RJREFUeJzt2rFt40AURdHVgo0oZU9K1QMrmCJU7DgmFBgO6DGuzokHxEsufsLbnPMf0PR/9QDgOgKHMIFDmMAhTOAQJnAIEziECRzCBA5h2xUfHWP4PQ4udhzH7bs3LjiECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmHb6gG/4TiO1RPePJ/P1RNO7vf76glcwAWHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BC2rR7wqV6v1+oJJ3PO1RP+vDHG6gk/5oJDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BC2rR7wG8YYqye8mXOunsAHcMEhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAjbVg/4VGOM1RNOjuNYPeHk8XisnvBm3/fVE37MBYcwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgELatHsDfMMZYPeFk3/fVExJccAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwm5zztUbgIu44BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUPYFxJaIJpC2XzHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09308749  0.14484791  0.08016486 -0.        ]\n",
      " [ 0.09308749  0.14484791  0.08016486 -0.        ]\n",
      " [ 0.09308749  0.14484791  0.08016486 -0.        ]\n",
      " [ 0.09308749  0.14484791  0.08016486 -0.        ]\n",
      " [ 0.09308749  0.14484791  0.08016486 -0.        ]\n",
      " [ 0.09308749  0.14484791  0.08016486 -0.        ]\n",
      " [ 0.09308749  0.14484791  0.08016486 -0.        ]\n",
      " [ 0.09308749  0.14484791  0.08016486 -0.        ]\n",
      " [ 0.09308749  0.14484791  0.08016486 -0.        ]\n",
      " [ 0.09308749  0.14484791  0.08016486 -0.        ]\n",
      " [ 0.09308749  0.14484791  0.08016486 -0.        ]\n",
      " [ 0.09308749  0.14484791  0.08016486 -0.        ]\n",
      " [ 0.09308749  0.14484791  0.08016486 -0.        ]\n",
      " [ 0.09308749  0.14484791  0.08016486 -0.        ]\n",
      " [ 0.09308749  0.14484791  0.08016486 -0.        ]\n",
      " [ 0.09308749  0.14484791  0.08016486 -0.        ]]\n",
      "[ 0.14484791  0.14484791  0.14484791  0.14484791  0.14484791  0.14484791\n",
      "  0.14484791  0.14484791  0.14484791  0.14484791  0.14484791  0.14484791\n",
      "  0.14484791  0.14484791  0.14484791  0.14484791]\n",
      "56.562\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Test preprocessing and estimator\n",
    "#\n",
    "\n",
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "e = Estimator(scope=\"test\")\n",
    "sp = StateProcessor()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Example observation batch\n",
    "    time_step = env.reset()\n",
    "    \n",
    "    frame = np.moveaxis(time_step.observation['RGB'], 0, -1)\n",
    "    observation_p = sp.process(sess, frame)\n",
    "\n",
    "    print(\"Sokoban in grey-scale:\")\n",
    "    print(observation_p)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(observation_p/255.0, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    observation = np.stack([observation_p] * FRAMES_STATE, axis=2)\n",
    "    observations = np.array([observation] * BATCH_SIZE)\n",
    "    \n",
    "    # Test Prediction\n",
    "    pred = e.predict(sess, observations)\n",
    "    print(pred)\n",
    "    print(pred.max(axis=1))\n",
    "\n",
    "    # Test training step\n",
    "    y = np.array([10.0, 4.0] * (BATCH_SIZE/2))\n",
    "    a = np.array([1, 3] * (BATCH_SIZE/2))\n",
    "    print(e.update(sess, observations, a, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train some agents! :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_model_parameters(sess, estimator1, estimator2):\n",
    "    \"\"\"\n",
    "    Copies the model parameters of one estimator to another.\n",
    "\n",
    "    Args:\n",
    "      sess: Tensorflow session instance\n",
    "      estimator1: Estimator to copy the paramters from\n",
    "      estimator2: Estimator to copy the parameters to\n",
    "    \"\"\"\n",
    "    e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "    e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "    e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "    e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "    update_ops = []\n",
    "    for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "        op = e2_v.assign(e1_v)\n",
    "        update_ops.append(op)\n",
    "\n",
    "    sess.run(update_ops)\n",
    "\n",
    "\n",
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn\n",
    "\n",
    "EpisodeStats = namedtuple(\"EpisodeStats\", [\"episode_lengths\", \"episode_rewards\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=50000,\n",
    "                    replay_memory_init_size=5000,\n",
    "                    update_target_estimator_every=1000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=50000,\n",
    "                    batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        sess: Tensorflow Session object\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        state_processor: A StateProcessor object\n",
    "        num_episodes: Number of episodes to run for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sampel when initializing \n",
    "          the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every N steps\n",
    "        discount_factor: Gamma discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "\n",
    "    Returns:\n",
    "        A generator\n",
    "        NOT ~ An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = EpisodeStats(episode_lengths=np.zeros(num_episodes),\n",
    "                         episode_rewards=np.zeros(num_episodes))\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "#     monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "#     if not os.path.exists(monitor_path):\n",
    "#         os.makedirs(monitor_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    \n",
    "    # Get the current time step\n",
    "    total_t = sess.run(tf.train.get_global_step())\n",
    "\n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(q_estimator, len(VALID_ACTIONS))\n",
    "\n",
    "    # DONE Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    time_step = env.reset()\n",
    "    frame = np.moveaxis(time_step.observation['RGB'], 0, -1)\n",
    "    state = state_processor.process(sess, frame)\n",
    "    state = np.stack([state] * FRAMES_STATE, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        probs = policy(sess, state, 0.1)\n",
    "        action = np.random.choice(VALID_ACTIONS, p=probs)\n",
    "\n",
    "        time_step = env.step(action)\n",
    "        frame = np.moveaxis(time_step.observation['RGB'], 0, -1)\n",
    "        next_state = state_processor.process(sess, frame)\n",
    "        next_state = np.stack([state[:,:,FRAMES_STATE - 1], next_state], axis=2)\n",
    "        done = time_step.last()\n",
    "        \n",
    "        replay_memory.append(Transition(state, action, time_step.reward, next_state, done))\n",
    "        if done:\n",
    "            time_step = env.reset()\n",
    "            frame = np.moveaxis(time_step.observation['RGB'], 0, -1)\n",
    "            state = state_processor.process(sess, frame)\n",
    "            state = np.stack([state] * FRAMES_STATE, axis=2)\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "            \n",
    "    for i_episode in range(num_episodes):\n",
    "\n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "\n",
    "        # Reset the environment\n",
    "        time_step = env.reset()\n",
    "        frame = np.moveaxis(time_step.observation['RGB'], 0, -1)\n",
    "        state = state_processor.process(sess, frame)\n",
    "        state = np.stack([state] * FRAMES_STATE, axis=2)\n",
    "        loss = None\n",
    "\n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "\n",
    "            # Add epsilon to Tensorboard\n",
    "            episode_summary = tf.Summary()\n",
    "            episode_summary.value.add(simple_value=epsilon, tag=\"epsilon\")\n",
    "            q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "\n",
    "            # DONE: Maybe update the target estimator\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                copy_model_parameters(sess, q_estimator, target_estimator)\n",
    "\n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Take a step in the environment\n",
    "            # DONE: Implement!\n",
    "            probs = policy(sess, state, epsilon)\n",
    "            action = np.random.choice(VALID_ACTIONS, p=probs)\n",
    "\n",
    "            time_step = env.step(action)\n",
    "            frame = np.moveaxis(time_step.observation['RGB'], 0, -1)\n",
    "            next_state = state_processor.process(sess, frame)\n",
    "            next_state = np.stack([state[:,:,FRAMES_STATE - 1], next_state], axis=2)\n",
    "            done = time_step.last()\n",
    "            \n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "\n",
    "            # DONE: Save transition to replay memory\n",
    "            replay_memory.append(Transition(state, action, time_step.reward, next_state, done))\n",
    "            \n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += time_step.reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "\n",
    "            # DONE: Sample a minibatch from the replay memory\n",
    "            sample = np.random.choice(len(replay_memory), batch_size)\n",
    "            sample = [replay_memory[i] for i in sample]\n",
    "            \n",
    "            sts, a, r, n_sts, d = tuple(map(np.array, zip(*sample)))\n",
    "            \n",
    "#             sts = np.array([replay_memory[i].state for i in sample])\n",
    "#             n_sts = np.array([replay_memory[i].next_state for i in sample])\n",
    "#             a = np.array([replay_memory[i].action for i in sample])\n",
    "#             r = np.array([replay_memory[i].reward for i in sample])\n",
    "#             d = np.array([replay_memory[i].done for i in sample])\n",
    "            \n",
    "            # DONE: Calculate q values and targets\n",
    "            qs = target_estimator.predict(sess, n_sts).max(axis=1)\n",
    "            qs[d] = 0\n",
    "            targets = r + discount_factor * qs\n",
    "            \n",
    "            # DONE: Perform gradient descent update\n",
    "            loss = q_estimator.update(sess, sts, a, targets)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "\n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], node_name=\"episode_reward\", tag=\"episode_reward\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], node_name=\"episode_length\", tag=\"episode_length\")\n",
    "        q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "        q_estimator.summary_writer.flush()\n",
    "\n",
    "        yield total_t, EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "\n",
    "    # env.close()\n",
    "    return  # stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model checkpoint /home/langust/Research/AIS/ai-camp-lpa/ai-safety-gridworlds/side_grids_camp/experiments/SideEffectsSokobanEnvironment/checkpoints/model...\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /home/langust/Research/AIS/ai-camp-lpa/ai-safety-gridworlds/side_grids_camp/experiments/SideEffectsSokobanEnvironment/checkpoints/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0408 14:22:38.872183 139915575940864 tf_logging.py:116] Restoring parameters from /home/langust/Research/AIS/ai-camp-lpa/ai-safety-gridworlds/side_grids_camp/experiments/SideEffectsSokobanEnvironment/checkpoints/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating replay memory...\n",
      "Step 43 (3645) @ Episode 20/5000, loss: 15.29858207787\n",
      "Episode Reward: 6.0\n",
      "Step 15 (4340) @ Episode 40/5000, loss: 84.7115020752\n",
      "Episode Reward: 34.0\n",
      "Step 28 (5091) @ Episode 60/5000, loss: 55.2191429138\n",
      "Episode Reward: 21.0\n",
      "Step 39 (5797) @ Episode 80/5000, loss: 156.059906006\n",
      "Episode Reward: 10.0\n",
      "Step 26 (6312) @ Episode 100/5000, loss: 78.1620941162\n",
      "Episode Reward: 23.0\n",
      "Step 12 (7078) @ Episode 120/5000, loss: 394.981750488\n",
      "Episode Reward: 37.0\n",
      "Step 8 (7729) @ Episode 140/5000, loss: 328.9420776374\n",
      "Episode Reward: 41.0\n",
      "Step 7 (8170) @ Episode 160/5000, loss: 179.4359741212\n",
      "Episode Reward: 42.0\n",
      "Step 43 (8716) @ Episode 180/5000, loss: 349.329864502\n",
      "Episode Reward: 6.0\n",
      "Step 10 (9200) @ Episode 200/5000, loss: 346.357055664\n",
      "Episode Reward: 39.0\n",
      "Step 23 (9732) @ Episode 220/5000, loss: 531.910949707\n",
      "Episode Reward: 26.0\n",
      "Step 24 (10211) @ Episode 240/5000, loss: 449.4223937998\n",
      "Episode Reward: 25.0\n",
      "Step 18 (10636) @ Episode 260/5000, loss: 656.1513061525\n",
      "Episode Reward: 31.0\n",
      "Step 24 (11081) @ Episode 280/5000, loss: 338.9859313965\n",
      "Episode Reward: 25.0\n",
      "Step 25 (11595) @ Episode 300/5000, loss: 392.0633544928\n",
      "Episode Reward: 24.0\n",
      "Step 27 (12000) @ Episode 320/5000, loss: 107.3885116584\n",
      "Episode Reward: 22.0\n",
      "Step 16 (12364) @ Episode 340/5000, loss: 103.00606536955\n",
      "Episode Reward: 33.0\n",
      "Step 18 (12709) @ Episode 360/5000, loss: 480.5405578616\n",
      "Episode Reward: 31.0\n",
      "Step 5 (13052) @ Episode 380/5000, loss: 206.50177002881\n",
      "Episode Reward: 44.0\n",
      "Step 22 (13481) @ Episode 400/5000, loss: 362.7664184576\n",
      "Episode Reward: 27.0\n",
      "Step 29 (13822) @ Episode 420/5000, loss: 694.0344848636\n",
      "Episode Reward: 20.0\n",
      "Step 11 (14136) @ Episode 440/5000, loss: 343.11203002982\n",
      "Episode Reward: 38.0\n",
      "Step 6 (14497) @ Episode 460/5000, loss: 0.40726435184524\n",
      "Episode Reward: 43.0\n",
      "Step 16 (14847) @ Episode 480/5000, loss: 319.6219177254\n",
      "Episode Reward: 33.0\n",
      "Step 12 (15175) @ Episode 500/5000, loss: 350.6740417489\n",
      "Episode Reward: 37.0\n",
      "Step 14 (15416) @ Episode 520/5000, loss: 498.9732666023\n",
      "Episode Reward: 35.0\n",
      "Step 5 (15604) @ Episode 540/5000, loss: 321.16180419981\n",
      "Episode Reward: 44.0\n",
      "Step 16 (16034) @ Episode 560/5000, loss: 303.3624877932\n",
      "Episode Reward: 33.0\n",
      "Step 5 (16355) @ Episode 580/5000, loss: 468.22787475674\n",
      "Episode Reward: 44.0\n",
      "Step 36 (16659) @ Episode 600/5000, loss: 199.3419189456\n",
      "Episode Reward: 13.0\n",
      "Step 22 (17122) @ Episode 620/5000, loss: 568.93493652365\n",
      "Episode Reward: 27.0\n",
      "Step 27 (17575) @ Episode 640/5000, loss: 215.72640991268\n",
      "Episode Reward: 22.0\n",
      "Step 16 (17868) @ Episode 660/5000, loss: 266.1150817873\n",
      "Episode Reward: 33.0\n",
      "Step 6 (18107) @ Episode 680/5000, loss: 324.278381348235\n",
      "Episode Reward: 43.0\n",
      "Step 9 (18351) @ Episode 700/5000, loss: 225.291625977134\n",
      "Episode Reward: 40.0\n",
      "Step 14 (18647) @ Episode 720/5000, loss: 225.02902221752\n",
      "Episode Reward: 35.0\n",
      "Step 9 (18903) @ Episode 740/5000, loss: 581.45483398473\n",
      "Episode Reward: 40.0\n",
      "Step 10 (19249) @ Episode 760/5000, loss: 197.06306457563\n",
      "Episode Reward: 39.0\n",
      "Step 16 (19453) @ Episode 780/5000, loss: 244.0786590581\n",
      "Episode Reward: 33.0\n",
      "Step 8 (19698) @ Episode 800/5000, loss: 239.655639648823\n",
      "Episode Reward: 41.0\n",
      "Step 14 (19954) @ Episode 820/5000, loss: 352.39508056669\n",
      "Episode Reward: 35.0\n",
      "Step 15 (20244) @ Episode 840/5000, loss: 268.83068847738\n",
      "Episode Reward: 34.0\n",
      "Step 4 (20491) @ Episode 860/5000, loss: 115.24298095729\n",
      "Episode Reward: 45.0\n",
      "Step 4 (20777) @ Episode 880/5000, loss: 459.221069336238\n",
      "Episode Reward: 45.0\n",
      "Step 10 (21024) @ Episode 900/5000, loss: 322.74194335945\n",
      "Episode Reward: 39.0\n",
      "Step 25 (21317) @ Episode 920/5000, loss: 125.59259033223\n",
      "Episode Reward: 24.0\n",
      "Step 10 (21543) @ Episode 940/5000, loss: 130.7480316166\n",
      "Episode Reward: 39.0\n",
      "Step 14 (21749) @ Episode 960/5000, loss: 357.43881225682\n",
      "Episode Reward: 35.0\n",
      "Step 10 (21976) @ Episode 980/5000, loss: 107.85235595764\n",
      "Episode Reward: 39.0\n",
      "Step 12 (22221) @ Episode 1000/5000, loss: 369.1203002935\n",
      "Episode Reward: 37.0\n",
      "Step 4 (22376) @ Episode 1020/5000, loss: 0.04333820194011\n",
      "Episode Reward: 45.0\n",
      "Step 11 (22611) @ Episode 1040/5000, loss: 225.108123779152\n",
      "Episode Reward: 38.0\n",
      "Step 17 (22860) @ Episode 1060/5000, loss: 105.66958618221\n",
      "Episode Reward: 32.0\n",
      "Step 11 (23085) @ Episode 1080/5000, loss: 98.88285827647\n",
      "Episode Reward: 38.0\n",
      "Step 9 (23262) @ Episode 1100/5000, loss: 377.71627807688\n",
      "Episode Reward: 40.0\n",
      "Step 12 (23551) @ Episode 1120/5000, loss: 226.29801940984\n",
      "Episode Reward: 37.0\n",
      "Step 6 (23776) @ Episode 1140/5000, loss: 215.905059814594\n",
      "Episode Reward: 43.0\n",
      "Step 7 (23989) @ Episode 1160/5000, loss: 307.664855957298\n",
      "Episode Reward: 42.0\n",
      "Step 6 (24167) @ Episode 1180/5000, loss: 125.193870544958\n",
      "Episode Reward: 43.0\n",
      "Step 6 (24412) @ Episode 1200/5000, loss: 357.030426025541\n",
      "Episode Reward: 43.0\n",
      "Step 12 (24604) @ Episode 1220/5000, loss: 364.1415405273\n",
      "Episode Reward: 37.0\n",
      "Step 6 (24785) @ Episode 1240/5000, loss: 457.593078613487\n",
      "Episode Reward: 43.0\n",
      "Step 7 (24972) @ Episode 1260/5000, loss: 104.413887024735\n",
      "Episode Reward: 42.0\n",
      "Step 16 (25155) @ Episode 1280/5000, loss: 452.07446289198\n",
      "Episode Reward: 33.0\n",
      "Step 8 (25386) @ Episode 1300/5000, loss: 136.939849854443\n",
      "Episode Reward: 41.0\n",
      "Step 7 (25601) @ Episode 1320/5000, loss: 466.95172119138\n",
      "Episode Reward: 42.0\n",
      "Step 8 (25780) @ Episode 1340/5000, loss: 108.914939884614\n",
      "Episode Reward: 41.0\n",
      "Step 15 (25971) @ Episode 1360/5000, loss: 455.95428466866\n",
      "Episode Reward: 34.0\n",
      "Step 21 (26155) @ Episode 1380/5000, loss: 228.79006958229\n",
      "Episode Reward: 28.0\n",
      "Step 7 (26329) @ Episode 1400/5000, loss: 0.08757964521652\n",
      "Episode Reward: 42.0\n",
      "Step 7 (26503) @ Episode 1420/5000, loss: 129.733093262512\n",
      "Episode Reward: 42.0\n",
      "Step 11 (26654) @ Episode 1440/5000, loss: 0.0998979061842\n",
      "Episode Reward: 38.0\n",
      "Step 6 (26805) @ Episode 1460/5000, loss: 228.592941284863\n",
      "Episode Reward: 43.0\n",
      "Step 4 (27011) @ Episode 1480/5000, loss: 0.12472726404759\n",
      "Episode Reward: 45.0\n",
      "Step 9 (27207) @ Episode 1500/5000, loss: 454.917419434155\n",
      "Episode Reward: 40.0\n",
      "Step 6 (27362) @ Episode 1520/5000, loss: 463.302551279846\n",
      "Episode Reward: 43.0\n",
      "Step 9 (27528) @ Episode 1540/5000, loss: 254.127273569852\n",
      "Episode Reward: 40.0\n",
      "Step 19 (27738) @ Episode 1560/5000, loss: 241.371093759276\n",
      "Episode Reward: 30.0\n",
      "Step 4 (27910) @ Episode 1580/5000, loss: 473.7403259283878\n",
      "Episode Reward: 45.0\n",
      "Step 10 (28116) @ Episode 1600/5000, loss: 454.847900391417\n",
      "Episode Reward: 39.0\n",
      "Step 7 (28254) @ Episode 1620/5000, loss: 254.368301392461\n",
      "Episode Reward: 42.0\n",
      "Step 8 (28396) @ Episode 1640/5000, loss: 211.940475464621\n",
      "Episode Reward: 41.0\n",
      "Step 4 (28551) @ Episode 1660/5000, loss: 230.816452026647\n",
      "Episode Reward: 45.0\n",
      "Step 12 (28732) @ Episode 1680/5000, loss: 230.948577881756\n",
      "Episode Reward: 37.0\n",
      "Step 8 (28911) @ Episode 1700/5000, loss: 123.6627655034153\n",
      "Episode Reward: 41.0\n",
      "Step 5 (29075) @ Episode 1720/5000, loss: 213.1860656747627\n",
      "Episode Reward: 44.0\n",
      "Step 7 (29194) @ Episode 1740/5000, loss: 229.610046387614\n",
      "Episode Reward: 42.0\n",
      "Step 12 (29369) @ Episode 1760/5000, loss: 106.59857940789\n",
      "Episode Reward: 37.0\n",
      "Step 16 (29508) @ Episode 1780/5000, loss: 121.668487549489\n",
      "Episode Reward: 33.0\n",
      "Step 17 (29679) @ Episode 1800/5000, loss: 129.58843994121\n",
      "Episode Reward: 32.0\n",
      "Step 5 (29818) @ Episode 1820/5000, loss: 245.85671997193\n",
      "Episode Reward: 44.0\n",
      "Step 6 (29963) @ Episode 1840/5000, loss: 248.416076662727\n",
      "Episode Reward: 43.0\n",
      "Step 8 (30147) @ Episode 1860/5000, loss: 251.4661560066615\n",
      "Episode Reward: 41.0\n",
      "Step 7 (30314) @ Episode 1880/5000, loss: 233.138778687149\n",
      "Episode Reward: 42.0\n",
      "Step 4 (30482) @ Episode 1900/5000, loss: 358.525238037638\n",
      "Episode Reward: 45.0\n",
      "Step 4 (30648) @ Episode 1920/5000, loss: 0.46932762861355\n",
      "Episode Reward: 45.0\n",
      "Step 9 (30811) @ Episode 1940/5000, loss: 354.256835938442\n",
      "Episode Reward: 40.0\n",
      "Step 7 (30979) @ Episode 1960/5000, loss: 265.017150879548\n",
      "Episode Reward: 42.0\n",
      "Step 6 (31120) @ Episode 1980/5000, loss: 216.437057495198\n",
      "Episode Reward: 43.0\n",
      "Step 6 (31288) @ Episode 2000/5000, loss: 353.405578613944\n",
      "Episode Reward: 43.0\n",
      "Step 8 (31444) @ Episode 2020/5000, loss: 106.994026184124\n",
      "Episode Reward: 41.0\n",
      "Step 5 (31587) @ Episode 2040/5000, loss: 121.63510894863\n",
      "Episode Reward: 44.0\n",
      "Step 15 (31734) @ Episode 2060/5000, loss: 230.6780242929\n",
      "Episode Reward: 34.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9 (31915) @ Episode 2080/5000, loss: 0.01539568509916\n",
      "Episode Reward: 40.0\n",
      "Step 8 (32067) @ Episode 2100/5000, loss: 0.0738589465618\n",
      "Episode Reward: 41.0\n",
      "Step 4 (32219) @ Episode 2120/5000, loss: 0.0438869073987\n",
      "Episode Reward: 45.0\n",
      "Step 8 (32383) @ Episode 2140/5000, loss: 103.979934692795\n",
      "Episode Reward: 41.0\n",
      "Step 10 (32536) @ Episode 2160/5000, loss: 0.1558234691625\n",
      "Episode Reward: 39.0\n",
      "Step 6 (32700) @ Episode 2180/5000, loss: 0.01353641692556\n",
      "Episode Reward: 43.0\n",
      "Step 5 (32828) @ Episode 2200/5000, loss: 593.895935059457\n",
      "Episode Reward: 44.0\n",
      "Step 6 (32976) @ Episode 2220/5000, loss: 0.01472977362571\n",
      "Episode Reward: 43.0\n",
      "Step 13 (33153) @ Episode 2240/5000, loss: 217.36465454156\n",
      "Episode Reward: 36.0\n",
      "Step 4 (33278) @ Episode 2260/5000, loss: 107.837745667518\n",
      "Episode Reward: 45.0\n",
      "Step 5 (33430) @ Episode 2280/5000, loss: 248.61456298873\n",
      "Episode Reward: 44.0\n",
      "Step 5 (33557) @ Episode 2300/5000, loss: 128.45898437559\n",
      "Episode Reward: 44.0\n",
      "Step 9 (33699) @ Episode 2320/5000, loss: 128.42021179227\n",
      "Episode Reward: 40.0\n",
      "Step 6 (33862) @ Episode 2340/5000, loss: 235.174835205526\n",
      "Episode Reward: 43.0\n",
      "Step 5 (33980) @ Episode 2360/5000, loss: 111.251960754288\n",
      "Episode Reward: 44.0\n",
      "Step 5 (34130) @ Episode 2380/5000, loss: 0.10290338099784\n",
      "Episode Reward: 44.0\n",
      "Step 5 (34267) @ Episode 2400/5000, loss: 112.065620422873\n",
      "Episode Reward: 44.0\n",
      "Step 8 (34420) @ Episode 2420/5000, loss: 232.187118532229\n",
      "Episode Reward: 41.0\n",
      "Step 6 (34540) @ Episode 2440/5000, loss: 500.219055176954\n",
      "Episode Reward: 43.0\n",
      "Step 4 (34672) @ Episode 2460/5000, loss: 256.510864258157\n",
      "Episode Reward: 45.0\n",
      "Step 5 (34800) @ Episode 2480/5000, loss: 112.585601807457\n",
      "Episode Reward: 44.0\n",
      "Step 10 (34930) @ Episode 2500/5000, loss: 359.0548706053\n",
      "Episode Reward: 39.0\n",
      "Step 11 (35073) @ Episode 2520/5000, loss: 349.690704346539\n",
      "Episode Reward: 38.0\n",
      "Step 4 (35197) @ Episode 2540/5000, loss: 126.258026123096\n",
      "Episode Reward: 45.0\n",
      "Step 11 (35340) @ Episode 2560/5000, loss: 0.0153825217858\n",
      "Episode Reward: 38.0\n",
      "Step 4 (35458) @ Episode 2580/5000, loss: 124.8628692636799\n",
      "Episode Reward: 45.0\n",
      "Step 4 (35596) @ Episode 2600/5000, loss: 109.195487976983\n",
      "Episode Reward: 45.0\n",
      "Step 8 (35726) @ Episode 2620/5000, loss: 217.252365112667\n",
      "Episode Reward: 41.0\n",
      "Step 4 (35860) @ Episode 2640/5000, loss: 207.361602783483\n",
      "Episode Reward: 45.0\n",
      "Step 4 (36000) @ Episode 2660/5000, loss: 227.1708679235819\n",
      "Episode Reward: 45.0\n",
      "Step 7 (36123) @ Episode 2680/5000, loss: 106.49714660687\n",
      "Episode Reward: 42.0\n",
      "Step 7 (36265) @ Episode 2700/5000, loss: 119.644889832528\n",
      "Episode Reward: 42.0\n",
      "Step 8 (36411) @ Episode 2720/5000, loss: 103.469001771413\n",
      "Episode Reward: 41.0\n",
      "Step 5 (36529) @ Episode 2740/5000, loss: 105.226837158839\n",
      "Episode Reward: 44.0\n",
      "Step 4 (36665) @ Episode 2760/5000, loss: 121.87476348984\n",
      "Episode Reward: 45.0\n",
      "Step 4 (36788) @ Episode 2780/5000, loss: 209.307998657943\n",
      "Episode Reward: 45.0\n",
      "Step 17 (36932) @ Episode 2800/5000, loss: 131.46492004469\n",
      "Episode Reward: 32.0\n",
      "Step 6 (37056) @ Episode 2820/5000, loss: 126.887649536444\n",
      "Episode Reward: 43.0\n",
      "Step 8 (37200) @ Episode 2840/5000, loss: 219.430236816971\n",
      "Episode Reward: 41.0\n",
      "Step 8 (37335) @ Episode 2860/5000, loss: 0.1639949828399\n",
      "Episode Reward: 41.0\n",
      "Step 10 (37456) @ Episode 2880/5000, loss: 109.1881790163\n",
      "Episode Reward: 39.0\n",
      "Step 11 (37588) @ Episode 2900/5000, loss: 111.12117767383\n",
      "Episode Reward: 38.0\n",
      "Step 4 (37696) @ Episode 2920/5000, loss: 381.694030762138\n",
      "Episode Reward: 45.0\n",
      "Step 7 (37826) @ Episode 2940/5000, loss: 0.07555106282235\n",
      "Episode Reward: 42.0\n",
      "Step 6 (37952) @ Episode 2960/5000, loss: 126.3091735846785\n",
      "Episode Reward: 43.0\n",
      "Step 4 (38071) @ Episode 2980/5000, loss: 0.06828080862763\n",
      "Episode Reward: 45.0\n",
      "Step 6 (38197) @ Episode 3000/5000, loss: 126.600761414613\n",
      "Episode Reward: 43.0\n",
      "Step 4 (38343) @ Episode 3020/5000, loss: 109.658119202723\n",
      "Episode Reward: 45.0\n",
      "Step 4 (38478) @ Episode 3040/5000, loss: 109.659606934499\n",
      "Episode Reward: 45.0\n",
      "Step 4 (38595) @ Episode 3060/5000, loss: 449.12658691443\n",
      "Episode Reward: 45.0\n",
      "Step 5 (38707) @ Episode 3080/5000, loss: 106.249900818934\n",
      "Episode Reward: 44.0\n",
      "Step 4 (38821) @ Episode 3100/5000, loss: 129.687301636337\n",
      "Episode Reward: 45.0\n",
      "Step 8 (38941) @ Episode 3120/5000, loss: 120.682250977038\n",
      "Episode Reward: 41.0\n",
      "Step 4 (39064) @ Episode 3140/5000, loss: 126.039382935143\n",
      "Episode Reward: 45.0\n",
      "Step 4 (39178) @ Episode 3160/5000, loss: 0.0623248144984\n",
      "Episode Reward: 45.0\n",
      "Step 4 (39299) @ Episode 3180/5000, loss: 0.0422306992114\n",
      "Episode Reward: 45.0\n",
      "Step 5 (39435) @ Episode 3200/5000, loss: 0.18341660499687\n",
      "Episode Reward: 44.0\n",
      "Step 6 (39549) @ Episode 3220/5000, loss: 128.280258179391\n",
      "Episode Reward: 43.0\n",
      "Step 5 (39658) @ Episode 3240/5000, loss: 0.07110143452889\n",
      "Episode Reward: 44.0\n",
      "Step 4 (39770) @ Episode 3260/5000, loss: 0.04378104954963\n",
      "Episode Reward: 45.0\n",
      "Step 12 (39906) @ Episode 3280/5000, loss: 252.62026977573\n",
      "Episode Reward: 37.0\n",
      "Step 6 (40017) @ Episode 3300/5000, loss: 368.415405273535\n",
      "Episode Reward: 43.0\n",
      "Step 5 (40126) @ Episode 3320/5000, loss: 123.601646423443\n",
      "Episode Reward: 44.0\n",
      "Step 8 (40241) @ Episode 3340/5000, loss: 232.385955811179\n",
      "Episode Reward: 41.0\n",
      "Step 5 (40351) @ Episode 3360/5000, loss: 249.484710693544\n",
      "Episode Reward: 44.0\n",
      "Step 6 (40494) @ Episode 3380/5000, loss: 211.6491546638958\n",
      "Episode Reward: 43.0\n",
      "Step 9 (40624) @ Episode 3400/5000, loss: 0.0424478352079\n",
      "Episode Reward: 40.0\n",
      "Step 4 (40739) @ Episode 3420/5000, loss: 0.03640754148366\n",
      "Episode Reward: 45.0\n",
      "Step 5 (40855) @ Episode 3440/5000, loss: 107.604187012409\n",
      "Episode Reward: 44.0\n",
      "Step 7 (40983) @ Episode 3460/5000, loss: 0.00558741297573\n",
      "Episode Reward: 42.0\n",
      "Step 6 (41086) @ Episode 3480/5000, loss: 235.39385986389\n",
      "Episode Reward: 43.0\n",
      "Step 6 (41188) @ Episode 3500/5000, loss: 133.086151123916\n",
      "Episode Reward: 43.0\n",
      "Step 4 (41302) @ Episode 3520/5000, loss: 0.00887263193727\n",
      "Episode Reward: 45.0\n",
      "Step 5 (41408) @ Episode 3540/5000, loss: 241.240249634044\n",
      "Episode Reward: 44.0\n",
      "Step 4 (41527) @ Episode 3560/5000, loss: 0.019724201411659\n",
      "Episode Reward: 45.0\n",
      "Step 4 (41639) @ Episode 3580/5000, loss: 254.126647949686\n",
      "Episode Reward: 45.0\n",
      "Step 5 (41744) @ Episode 3600/5000, loss: 131.063674927272\n",
      "Episode Reward: 44.0\n",
      "Step 4 (41865) @ Episode 3620/5000, loss: 0.13775780797799\n",
      "Episode Reward: 45.0\n",
      "Step 8 (41983) @ Episode 3640/5000, loss: 243.137527466115\n",
      "Episode Reward: 41.0\n",
      "Step 5 (42096) @ Episode 3660/5000, loss: 0.06872579455383\n",
      "Episode Reward: 44.0\n",
      "Step 4 (42207) @ Episode 3680/5000, loss: 0.15615561604564\n",
      "Episode Reward: 45.0\n",
      "Step 5 (42328) @ Episode 3700/5000, loss: 107.592796326348\n",
      "Episode Reward: 44.0\n",
      "Step 4 (42437) @ Episode 3720/5000, loss: 0.0430820472538\n",
      "Episode Reward: 45.0\n",
      "Step 6 (42546) @ Episode 3740/5000, loss: 105.86721801897\n",
      "Episode Reward: 43.0\n",
      "Step 4 (42670) @ Episode 3760/5000, loss: 104.25695037831\n",
      "Episode Reward: 45.0\n",
      "Step 10 (42779) @ Episode 3780/5000, loss: 110.56388092988\n",
      "Episode Reward: 39.0\n",
      "Step 6 (42895) @ Episode 3800/5000, loss: 137.092971802144\n",
      "Episode Reward: 43.0\n",
      "Step 4 (42995) @ Episode 3820/5000, loss: 101.984008789333\n",
      "Episode Reward: 45.0\n",
      "Step 4 (43111) @ Episode 3840/5000, loss: 0.0274535510689\n",
      "Episode Reward: 45.0\n",
      "Step 8 (43221) @ Episode 3860/5000, loss: 130.017868042537\n",
      "Episode Reward: 41.0\n",
      "Step 4 (43322) @ Episode 3880/5000, loss: 0.0666110068563\n",
      "Episode Reward: 45.0\n",
      "Step 4 (43427) @ Episode 3900/5000, loss: 0.05328953266149\n",
      "Episode Reward: 45.0\n",
      "Step 4 (43534) @ Episode 3920/5000, loss: 236.25712585408\n",
      "Episode Reward: 45.0\n",
      "Step 4 (43640) @ Episode 3940/5000, loss: 356.713623047246\n",
      "Episode Reward: 45.0\n",
      "Step 9 (43753) @ Episode 3960/5000, loss: 214.182052612625\n",
      "Episode Reward: 40.0\n",
      "Step 6 (43852) @ Episode 3980/5000, loss: 0.20237536728483\n",
      "Episode Reward: 43.0\n",
      "Step 4 (43958) @ Episode 4000/5000, loss: 123.693107605834\n",
      "Episode Reward: 45.0\n",
      "Step 4 (44068) @ Episode 4020/5000, loss: 124.117889404849\n",
      "Episode Reward: 45.0\n",
      "Step 7 (44174) @ Episode 4040/5000, loss: 0.05437215417627\n",
      "Episode Reward: 42.0\n",
      "Step 5 (44274) @ Episode 4060/5000, loss: 0.015887370333187\n",
      "Episode Reward: 44.0\n",
      "Step 5 (44382) @ Episode 4080/5000, loss: 109.5211639435344\n",
      "Episode Reward: 44.0\n",
      "Step 5 (44497) @ Episode 4100/5000, loss: 263.4594726562929\n",
      "Episode Reward: 44.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6 (44592) @ Episode 4120/5000, loss: 0.02477198094132\n",
      "Episode Reward: 43.0\n",
      "Step 9 (44699) @ Episode 4140/5000, loss: 0.04212981835014\n",
      "Episode Reward: 40.0\n",
      "Step 7 (44802) @ Episode 4160/5000, loss: 0.09070052206523\n",
      "Episode Reward: 42.0\n",
      "Step 4 (44895) @ Episode 4180/5000, loss: 125.792404175725\n",
      "Episode Reward: 45.0\n",
      "Step 4 (45014) @ Episode 4200/5000, loss: 0.7152706384662194\n",
      "Episode Reward: 45.0\n",
      "Step 4 (45138) @ Episode 4220/5000, loss: 353.8522033691457\n",
      "Episode Reward: 45.0\n",
      "Step 7 (45240) @ Episode 4240/5000, loss: 0.5937678813934\n",
      "Episode Reward: 42.0\n",
      "Step 4 (45355) @ Episode 4260/5000, loss: 116.02014923176\n",
      "Episode Reward: 45.0\n",
      "Step 14 (45465) @ Episode 4280/5000, loss: 121.61132812525\n",
      "Episode Reward: 35.0\n",
      "Step 4 (45584) @ Episode 4300/5000, loss: 352.309844971943\n",
      "Episode Reward: 45.0\n",
      "Step 4 (45688) @ Episode 4320/5000, loss: 227.15048217814\n",
      "Episode Reward: 45.0\n",
      "Step 4 (45785) @ Episode 4340/5000, loss: 120.830200195665\n",
      "Episode Reward: 45.0\n",
      "Step 6 (45884) @ Episode 4360/5000, loss: 0.02133821323512\n",
      "Episode Reward: 43.0\n",
      "Step 5 (45991) @ Episode 4380/5000, loss: 0.08487326651814\n",
      "Episode Reward: 44.0\n",
      "Step 4 (46088) @ Episode 4400/5000, loss: 235.499893188532\n",
      "Episode Reward: 45.0\n",
      "Step 4 (46198) @ Episode 4420/5000, loss: 219.466857918252\n",
      "Episode Reward: 45.0\n",
      "Step 4 (46291) @ Episode 4440/5000, loss: 0.047968152910522\n",
      "Episode Reward: 45.0\n",
      "Step 7 (46394) @ Episode 4460/5000, loss: 0.07477141916753\n",
      "Episode Reward: 42.0\n",
      "Step 4 (46492) @ Episode 4480/5000, loss: 0.00784385204315\n",
      "Episode Reward: 45.0\n",
      "Step 5 (46582) @ Episode 4500/5000, loss: 126.97682189956\n",
      "Episode Reward: 44.0\n",
      "Step 4 (46687) @ Episode 4520/5000, loss: 235.259063721973\n",
      "Episode Reward: 45.0\n",
      "Step 4 (46777) @ Episode 4540/5000, loss: 0.07844361662863\n",
      "Episode Reward: 45.0\n",
      "Step 4 (46867) @ Episode 4560/5000, loss: 0.04498368501666\n",
      "Episode Reward: 45.0\n",
      "Step 6 (46974) @ Episode 4580/5000, loss: 108.1760482797186\n",
      "Episode Reward: 43.0\n",
      "Step 6 (47080) @ Episode 4600/5000, loss: 111.047080994675\n",
      "Episode Reward: 43.0\n",
      "Step 5 (47176) @ Episode 4620/5000, loss: 0.02702254988255\n",
      "Episode Reward: 44.0\n",
      "Step 4 (47266) @ Episode 4640/5000, loss: 0.01519392058259\n",
      "Episode Reward: 45.0\n",
      "Step 6 (47359) @ Episode 4660/5000, loss: 0.167824864388535\n",
      "Episode Reward: 43.0\n",
      "Step 4 (47461) @ Episode 4680/5000, loss: 0.00115760904737\n",
      "Episode Reward: 45.0\n",
      "Step 4 (47561) @ Episode 4700/5000, loss: 0.853429734707878\n",
      "Episode Reward: 45.0\n",
      "Step 5 (47658) @ Episode 4720/5000, loss: 0.002427726984026\n",
      "Episode Reward: 44.0\n",
      "Step 4 (47753) @ Episode 4740/5000, loss: 109.4944839480752\n",
      "Episode Reward: 45.0\n",
      "Step 4 (47845) @ Episode 4760/5000, loss: 108.8582382203134\n",
      "Episode Reward: 45.0\n",
      "Step 4 (47939) @ Episode 4780/5000, loss: 0.006958981044599\n",
      "Episode Reward: 45.0\n",
      "Step 6 (48043) @ Episode 4800/5000, loss: 0.251504153013594\n",
      "Episode Reward: 43.0\n",
      "Step 4 (48140) @ Episode 4820/5000, loss: 0.004358638077972\n",
      "Episode Reward: 45.0\n",
      "Step 4 (48234) @ Episode 4840/5000, loss: 0.003037442453212\n",
      "Episode Reward: 45.0\n",
      "Step 6 (48324) @ Episode 4860/5000, loss: 0.13196054101927\n",
      "Episode Reward: 43.0\n",
      "Step 7 (48414) @ Episode 4880/5000, loss: 0.001982888206844\n",
      "Episode Reward: 42.0\n",
      "Step 4 (48506) @ Episode 4900/5000, loss: 0.059883799403983\n",
      "Episode Reward: 45.0\n",
      "Step 5 (48592) @ Episode 4920/5000, loss: 0.01613296009655\n",
      "Episode Reward: 44.0\n",
      "Step 4 (48686) @ Episode 4940/5000, loss: 0.00646727578714\n",
      "Episode Reward: 45.0\n",
      "Step 5 (48778) @ Episode 4960/5000, loss: 102.999412537849\n",
      "Episode Reward: 44.0\n",
      "Step 4 (48868) @ Episode 4980/5000, loss: 0.00990272499621\n",
      "Episode Reward: 45.0\n",
      "Step 4 (48965) @ Episode 5000/5000, loss: 0.014333701692564\n",
      "Episode Reward: 45.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(type(env).__name__))\n",
    "\n",
    "# Create a glboal step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "# Create estimators\n",
    "q_estimator = Estimator(scope=\"q\", summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope=\"target_q\")\n",
    "\n",
    "# State processor\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "# Run it!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t, stats in deep_q_learning(sess,\n",
    "                                    env,\n",
    "                                    q_estimator=q_estimator,\n",
    "                                    target_estimator=target_estimator,\n",
    "                                    state_processor=state_processor,\n",
    "                                    experiment_dir=experiment_dir,\n",
    "                                    num_episodes=5000,  # 5000\n",
    "                                    replay_memory_size=500,  # 50000\n",
    "                                    replay_memory_init_size=5000,  # 5000\n",
    "                                    update_target_estimator_every=250,  # 1000\n",
    "                                    epsilon_start=1.0,\n",
    "                                    epsilon_end=0.1,\n",
    "                                    epsilon_decay_steps=50000,\n",
    "                                    discount_factor=0.99,\n",
    "                                    batch_size=BATCH_SIZE):\n",
    "        if len(stats.episode_rewards) % 20 == 0:\n",
    "            print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode rewards:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode lengths:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Episode rewards:\")\n",
    "plt.plot(stats.episode_rewards)\n",
    "plt.show()\n",
    "print(\"Episode lengths:\")\n",
    "plt.plot(stats.episode_lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(stats, open(experiment_dir + \"/stats.p\", \"wb\" ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py27)",
   "language": "python",
   "name": "conda_py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
